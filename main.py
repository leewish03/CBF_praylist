from google_sheets import get_prayer_requests
from data_processor import process_prayer_requests
from notion_publisher import publish_to_notion
from utils import retry_on_failure, validate_environment_variables, PipelineError, APIConnectionError
from config import config
import logging
import logging.handlers
import os
import traceback
import sys
from datetime import datetime

def setup_logging():
    """í–¥ìƒëœ ë¡œê¹… ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = os.path.dirname(config.logging.file_path)
    if log_dir and not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, config.logging.level))
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ë¡œí…Œì´ì…˜)
    file_handler = logging.handlers.RotatingFileHandler(
        config.logging.file_path,
        maxBytes=config.logging.max_file_size * 1024 * 1024,  # MB to bytes
        backupCount=config.logging.backup_count,
        encoding='utf-8'
    )
    file_handler.setLevel(getattr(logging, config.logging.level))
    file_formatter = logging.Formatter(config.logging.format)
    file_handler.setFormatter(file_formatter)
    root_logger.addHandler(file_handler)

@retry_on_failure(max_retries=3, delay=2.0)
def fetch_data_with_retry():
    """ì¬ì‹œë„ ë¡œì§ì´ ì ìš©ëœ ë°ì´í„° ìˆ˜ì§‘"""
    logger = logging.getLogger(__name__)
    logger.info("êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    
    try:
        df = get_prayer_requests()
        if df is None:
            raise APIConnectionError("êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê°œ í–‰")
        return df
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        raise APIConnectionError(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

@retry_on_failure(max_retries=2, delay=3.0)  
def publish_with_retry(processed_data):
    """ì¬ì‹œë„ ë¡œì§ì´ ì ìš©ëœ Notion ê²Œì‹œ"""
    logger = logging.getLogger(__name__)
    logger.info("Notionì— ë°ì´í„° ê²Œì‹œ ì‹œì‘")
    
    try:
        publish_to_notion(processed_data)
        logger.info("Notion ê²Œì‹œ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"Notion ê²Œì‹œ ì‹¤íŒ¨: {str(e)}")
        raise APIConnectionError(f"Notion ê²Œì‹œ ì‹¤íŒ¨: {str(e)}")

def generate_pipeline_report(processed_data, execution_time):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë³´ê³ ì„œ ìƒì„±"""
    logger = logging.getLogger(__name__)
    
    total_prayers = sum(len(prayers) for prayers in processed_data['prayers_by_requester'].values())
    total_requesters = len(processed_data['prayers_by_requester'])
    
    report = f"""
ğŸ“Š CBF ê¸°ë„ì œëª© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë³´ê³ ì„œ
{'='*50}
ğŸ• ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
â±ï¸  ì²˜ë¦¬ ì†Œìš”ì‹œê°„: {execution_time:.2f}ì´ˆ
ğŸ“ ì´ ê¸°ë„ì œëª© ìˆ˜: {total_prayers}ê°œ
ğŸ‘¥ ì œì¶œì ìˆ˜: {total_requesters}ëª…
ğŸ“‹ ë‹´ë‹¹ìë³„ ë¶„ë°°:
"""
    
    from config import PrayerAssignments
    assignments = PrayerAssignments.get_assignments()
    
    for manager, assignees in assignments.items():
        manager_total = sum(
            len(processed_data['prayers_by_requester'].get(assignee, []))
            for assignee in assignees
        )
        report += f"   ğŸ“Œ {manager}: {manager_total}ê°œ\n"
    
    # ë””ë²„ê¹…: ì‹¤ì œ ë°ì´í„°ì™€ ë§¤í•‘ ë¹„êµ
    all_assignees = PrayerAssignments.get_all_assignees()
    actual_names = list(processed_data['prayers_by_requester'].keys())
    
    logger.info(f"ğŸ” ë””ë²„ê¹… ì •ë³´:")
    logger.info(f"   ì‹¤ì œ ë°ì´í„° ì´ë¦„ë“¤: {actual_names}")
    logger.info(f"   ë§¤í•‘ëœ assignees: {all_assignees}")
    
    # ê° ì´ë¦„ì— ëŒ€í•´ ìƒì„¸ ë¹„êµ
    for name in actual_names:
        if name not in all_assignees:
            logger.warning(f"   âŒ '{name}' (ê¸¸ì´: {len(name)}, repr: {repr(name)}) - ë§¤í•‘ì— ì—†ìŒ")
        else:
            logger.info(f"   âœ… '{name}' - ë§¤í•‘ì— ìˆìŒ")
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ì œì¶œì í™•ì¸
    unmapped = [name for name in actual_names if name not in all_assignees]
    
    if unmapped:
        report += f"\nâš ï¸  ë‹´ë‹¹ì ë¯¸ì§€ì •: {', '.join(unmapped)}"
    
    report += f"\nâœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ"
    
    return report

def run_pipeline():
    """ê°œì„ ëœ ë©”ì¸ íŒŒì´í”„ë¼ì¸"""
    logger = logging.getLogger(__name__)
    start_time = datetime.now()
    
    try:
        logger.info("="*50)
        logger.info("CBF ê¸°ë„ì œëª© ìë™í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("="*50)
        
        # 1. ì„¤ì • ê²€ì¦
        logger.info("1ï¸âƒ£ ì„¤ì • ë° í™˜ê²½ë³€ìˆ˜ ê²€ì¦")
        config.validate()
        validate_environment_variables()
        
        # 2. ë°ì´í„° ìˆ˜ì§‘
        logger.info("2ï¸âƒ£ êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„° ìˆ˜ì§‘")
        df = fetch_data_with_retry()
        
        # 3. ë°ì´í„° ì²˜ë¦¬
        logger.info("3ï¸âƒ£ ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜")
        processed_data = process_prayer_requests(df)
        if processed_data is None:
            raise PipelineError("ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
        
        # 4. Notion ê²Œì‹œ
        logger.info("4ï¸âƒ£ Notion í˜ì´ì§€ ì—…ë°ì´íŠ¸")
        publish_with_retry(processed_data)
        
        # 5. ì‹¤í–‰ ë³´ê³ ì„œ
        execution_time = (datetime.now() - start_time).total_seconds()
        report = generate_pipeline_report(processed_data, execution_time)
        
        logger.info("5ï¸âƒ£ ì‹¤í–‰ ì™„ë£Œ")
        logger.info(report)
        
        return True
        
    except ValueError as e:
        logger.error(f"ì„¤ì • ì˜¤ë¥˜: {str(e)}")
        return False
    except APIConnectionError as e:
        logger.error(f"API ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        return False
    except PipelineError as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")
        return False
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return False
    finally:
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ (ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ)")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    setup_logging()
    logger = logging.getLogger(__name__)
    
    try:
        success = run_pipeline()
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(0)
    except Exception as e:
        logger.critical(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()